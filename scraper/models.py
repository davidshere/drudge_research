import socket
import uuid

from bs4 import BeautifulSoup
import requests

from grab_main_and_splash import get_main_and_splash

EMPTY_PAGE_LINK_COUNT = 10

class DrudgeBase(object):
    def fetch_page(self):
        ''' Fetches the html from a URL. Assumes the subclass
            has a self.url attribute 
        '''
        request = requests.get(self.url)
        html = request.text
        return BeautifulSoup(html, 'lxml')

class DayPage(DrudgeBase):
    ''' Signifies a date, in UTC time.
        Generated by day_page_list_generator()

        It's main purpose is to represent a day page and scrape
        and process each iteration of the drudge report that day.
    '''
    def __init__(self, day_page_url, drudge_date):
        self.url = day_page_url
        self.drudge_date = drudge_date

    def drudge_pages(self):
        '''Goes through a day page and scrapes the links to the 
        individual drudge pages'''
        soup = self.fetch_page()
        url_front = 'http://www.drudgereportArchives.com/data/'
        all_links = soup.find_all('a')
        for link in all_links:
            url = link['href']
            url_text = link.text.encode('utf-8').strip()
            if url.startswith(url_front) and url_text != '^':
                yield DrudgePage(
                    url=url,
                    time_str=url_text
                )

    def get_days_links(self):
        page_links = []
        for i, page in enumerate(self.drudge_pages()):
            print(i, page.url)
            links = [link for link in page.scrape()]
            page_links.extend([link for link in page.scrape()])
        return page_links


class DrudgePage(DrudgeBase):
    '''
        One method to scrape the drudge page and produce a list(?) of drudge link objects
    '''
    
    socket.setdefaulttimeout(5)

    def __init__(self, url, time_str): 
        self.url = url
        self.time_str = time_str
        self.link_count = None
        self.id = uuid.uuid4().hex

    def scrape(self):
        ''' scrape takes a url to an individual drudge page, and 
            scrapes every link.  '''        
        soup = self.fetch_page()
        raw_links = soup.find_all('a')
        if self._page_has_content(raw_links):
            main_links = get_main_and_splash(soup)
            for link in raw_links:

                splash = False
                top = False
                url = link['href']
                text = link.text.encode('utf-8')

                if link.text == main_links['splash']:
                    splash = True
                if link.text in main_links['top']:
                    top = True

                yield DrudgeLink(url, text, self.id, top, splash)

    def _page_has_content(self, links):
        """ Returns true if a drudge page has actual content. """
        return len(links) >= EMPTY_PAGE_LINK_COUNT

class DrudgeLink(object):
    def __init__(self, 
                 url, 
                 hed, 
                 parent_drudge_page_id,
                 is_top = False, 
                 is_splash = False):
        self.url = url
        self.hed = hed
        self.is_top = is_top
        self.is_splash = is_splash
        self.parent_drudge_page_id = parent_drudge_page_id

    def dump(self):
        return [self.url,
                self.hed,
                self.top,
                self.splash,
                self.parent_drudge_page_id
        ]

if __name__ == "__main__":
    from archive_to_s3 import day_pages
    one = next(day_pages())
    links = one.get_days_links()
    print(links[:100])
