import aiohttp
import asyncio
import collections
import uuid

from bs4 import BeautifulSoup
import requests

from grab_main_and_splash import get_main_and_splash

DAY_PAGE_FMT_URL = "http://www.drudgereportarchives.com/data/%s/%02d/%02d/index.htm?s=flag"
EMPTY_PAGE_LINK_COUNT = 15

DRUDGE_LINK_FIELD_NAMES = ['url', 'hed', 'is_top', 'is_splash', 'parent_drudge_page_id']
DrudgeLink = collections.namedtuple("DrudgeLink", DRUDGE_LINK_FIELD_NAMES)

class DrudgeBase(object):

    async def fetch_page(self):
        ''' Fetches the html from a URL. Assumes the subclass
            has a self.url attribute 
        '''
        async with aiohttp.ClientSession() as session:
            async with session.get(self.url) as response:
                response = await response.read()

        return BeautifulSoup(response, 'lxml')

class DayPage(DrudgeBase):
    ''' Signifies a date, in UTC time.
        Generated by day_page_list_generator()

        It's main purpose is to represent a day page and scrape
        and process each iteration of the drudge report that day.
    '''
    def __init__(self, dt):
        self.url = DAY_PAGE_FMT_URL % (dt.year, dt.month, dt.day)
        self.dt = dt

    def drudge_pages(self):
        '''Goes through a day page and scrapes the links to the 
        individual drudge pages'''
        soup = self.fetch_page()
        url_front = 'http://www.drudgereportArchives.com/data/'
        all_links = soup.find_all('a')
        for link in all_links:
            url = link['href']
            url_text = link.text.encode('utf-8').strip()
            if url.startswith(url_front) and url_text != '^':
                yield DrudgePage(
                    url=url,
                    time_str=url_text
                )

    def get_days_links(self):
        page_links = []
        for i, page in enumerate(self.drudge_pages()):
            print(i, page.url)
            links = [link for link in page.get_links()]
            page_links.extend([link for link in page.scrape()])
        return page_links


class DrudgePage(DrudgeBase):
    '''
        One method to scrape the drudge page and produce an iterator of drudge link objects
    '''
    def __init__(self, url, time_str): 
        self.url = url
        self.time_str = time_str
        self.id = uuid.uuid4().hex

    def get_links(self):
        ''' scrape takes a url to an individual drudge page, and 
            scrapes every link.  '''        
        soup = self.fetch_page()
        raw_links = soup.find_all('a')
        if self._page_has_content(raw_links):
            main_links = get_main_and_splash(soup)
            for link in raw_links:
                splash = False
                top = False

                url = link['href']
                text = link.text.encode('utf-8')

                if link.text == main_links['splash']:
                    splash = True
                if link.text in main_links['top']:
                    top = True

                yield DrudgeLink(url, text, self.id, top, splash)

    def _page_has_content(self, links):
        """ Returns true if a drudge page has actual content. """
        return len(links) > EMPTY_PAGE_LINK_COUNT


if __name__ == "__main__":
    from archive_to_s3 import day_pages
    one = next(day_pages())
    links = one.get_days_links()
    print(links[:100])

