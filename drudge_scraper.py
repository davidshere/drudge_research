from bs4 import BeautifulSoup

import csv
from datetime import date
import re
import socket
from sys import argv
from time import gmtime
from urllib2 import urlopen

TEMP_DAYS_FILENAME = 'day.csv'
URL_FORMAT_STRING = "http://www.drudgereportarchives.com/data/%d/%02d/%02d/index.htm?s=flag"

socket.setdefaulttimeout(5)


class DrudgeBase(object):

    def fetch_page(self, url):
        success = False
        attempt_count = 0
        html = urlopen(url).read()
        return html 

class DayPage(DrudgeBase):
    ''' Signifies a date, in UTC time.
        Generated by day_page_list_generator()

        scrape_day_page


    '''
    def __init__(self, day_page_url, drudge_date, day_order):
        self.url = day_page_url
        self.drudge_date = drudge_date
        self.day_order = day_order


    def scrape_day_page(self):
        '''Goes through a day page and scrapes the links to the 
        individual drudge pages'''
        html = self.fetch_page(self.url)
        url_front = 'http://www.drudgereportArchives.com/data/'
        soup = BeautifulSoup(html, 'lxml')
        all_urls = soup.find_all('a')
        ## add a BS functtion that gets all of the link text
        drudge_url_list = [{'url':url['href'], 
                            'time':url.text.encode('utf-8').strip(), 
                            'drudge_date':self.drudge_date,
                            'day_order':self.day_order} 
                            for url in all_urls if 
                                url['href'][0:41] == url_front]
                                
        drudge_url_list = [entry for entry in drudge_url_list if entry['time']!="^"]
        return [DrudgePage(self, 
                           drudge_page_data['url'], 
                           drudge_page_data['time'])
                           for drudge_page_data in drudge_url_list]
        
class DrudgePage(DrudgeBase):

    socket.setdefaulttimeout(5)
    
    def __init__(self, day_page_obj, url, time): 
        self.url = url
        self.time = time
        self.drudge_links = []
        self.link_count = None
        self.id = id(self)  


        
    @classmethod 
    def _hed_and_splash_finder():
      # if time/date > Oct 6, 2009 05:57:42 EXT (10:57:42 GMT)
      try:
        top = self.soup.find('div', {'id': 'drudgeTopHeadlines'})
        top = top.find_all('a')
        top = [link.text.encode('utf-8') for link in top]
        splash = top.pop()
        return (top, splash)
      except:
        return (None, None)

    def drudge_page_scraper(self):
        ''' drudge_page_scraper takes a url to an individual drudge page, and 
            scrapes every link.  '''               
        html = self.fetch_page(self.url)
        soup = BeautifulSoup(html, 'lxml')
        try:
            #finding the splash and top headlines       
            top = soup.find(id='drudgeTopHeadlines')
            top = top.find_all('a')
            top = [link.text.encode('utf-8') for link in top]
            splash_text = ""
            splash_text = top.pop()
            main_text = set(top)
        except:
            splash_text = ""
            main_text = ""        

        links = soup.find_all('a')
        return [DrudgeLink(link['href'],link.text.encode('utf-8'), self.id) for link in links]
    
class DrudgeLink(object):
    def __init__(self, 
                 url, 
                 hed, 
                 parent_drudge_page_id,
                 main=False, 
                 splash = False, ):
        self.url = url
        self.hed = hed
        self.main = main
        self.splash = splash
        self.parent_drudge_page_id = parent_drudge_page_id

    def list_dump(self):
        return [self.url,
                self.hed,
                self.main,
                self.splash,
                self.parent_drudge_page_id
        ]

def day_page_list_generator(start = {}, end = {}):
    '''Returns a list of urls to for each day page in the drudge Archive 
    either between the beginning of the archive until today or from
    a specified start date to a specified end date. The date take the for of a
    dictionary, like this:
    
                {'year': year, 'month': month, 'day': day}
    
    '''
    # initializing variables, years, months, and days
    year_list = range(2001, gmtime()[0]+1)
    day_order = -321 # because the function makes urls for 321 days before the
                     # first day page in the drudge archive                    
    month_days = [(1,31), (2, 28), (3, 31), (4, 30), (5, 31), (6, 30), (7, 31),
                 (8, 31), (9, 30), (10, 31), (11, 30), (12, 31)]
    full_day_page_list = []
    trimmed_day_page_list = []
    
    # creating the full range of day_page links
    for year in year_list:
        if year % 4 == 0:
            leap_year = True
        else:
            leap_year = False  
        for month in month_days:
            if month[0]==2 and leap_year: #check if it's a leap_year
                month = (2, 29)
            for day in range(1, month[1]+1):
                url = URL_FORMAT_STRING % (year, month[0], day)
                one_date = date(year, month[0], day)
                day_order += 1
                response = (one_date, DayPage(url, one_date, day_order)) # making a tuple with the date and day page object
                full_day_page_list.append(response) # adding it to the list of day page objects    
                           
    # taking the specific day page links requested, or all of them if no range is specified
    today = date.today() # setting a today variable with the current year, month, and day
    if start:
        start = date(start['year'], start['month'], start['day'])
    else:
        start = date(2001, 11, 18)  
    if end:
        end = date(end['year'], end['month'], end['day'])
    else:
        end = date(today.year, today.month, today.day)

    # ensure start_date is before end_date
    assert start <= end, "start date must be before end date"
    
    started = False
    final_day_page_list = []
    for page in full_day_page_list:
        if page[0] == start:
            started = True
        if started:
            trimmed_day_page_list.append(page[1])
            if page[0] == end:
                return trimmed_day_page_list


'''
UTILITIES
'''

def dump_day_pages_to_file(list_of_day_pages, filename):
    with open(filename, 'w') as f:
        writer = csv.writer(f)      
        for day in list_of_day_pages:
            writer.writerow([day.day_order,
                             day.drudge_date,
                             day.url])

def one_drudge_page_dump():
    ''' scrapes the most recent drudge page in the archive, outputs the results
    to a .csv file '''
    day_page_list = day_page_list_generator()
    sample_drudge_pages = day_page_list[0].scrape_day_page() # scrape it, get a list of drudge page objects
    sample_drudge_page =  sample_drudge_pages[-1] # grab the most recent drudge page object
    drudge_link_objects = sample_drudge_page.drudge_page_scraper() # scrape it, get drudge link objects
    for link in drudge_link_objects:
        print link
        print
    with open('drudge_page.csv', 'w') as page:
        writer = csv.writer(page)
        
        for test in drudge_link_objects:
            writer.writerow(test.list_dump())
    return drudge_link_objects

def scrape_archive():
    '''
    Iterates through a list of day pages, then fetches the drudge pages, then
    fetches the links. Creates DayPage objects, DrudgePage objects, and
    DrudgeLink objects. 

    For now it will write them all to a .csv file. Soon it will write them to
    a database where postprocessing and data analysis will be performed.

    '''
    # generate a list of day_pages within start and end dates, if given
    start_date = {'year': 2014, 'month':12, 'day': 3}
    end_date = {'year':2014, 'month':12, 'day': 7}
    day_pages = day_page_list_generator(start=start_date, end=end_date) 
    link_header_row = ['url',  'hed', 'main', 'splash',  'parent_drudge_page_id']
    day_page_header_row = []
    drudge_page_header_row []    
    #open a connection a .csv file to dump the data
    with open('drudge_page.csv', 'w') as page:
        writer = csv.writer(page)   
        writer.writerow(header_row)

        for day in day_pages:
            #scrape each day page
            drudge_pages = day.scrape_day_page()
            #iterate through my list of drudge_pages
            for index, drudge_page in enumerate(drudge_pages):
                #scrape each drudge page for drudge_links
                drudge_links = drudge_page.drudge_page_scraper()
                print index, "out of", len(drudge_pages)
                #iterate through each drudge page's drudge links, write data to .csv file
                for link in drudge_links:                   
                    writer.writerow(link.list_dump())
               


'''
For development, not for keeping
'''
day = day_page_list_generator()[0]
page = day.scrape_day_page()[0]
link = page.drudge_page_scraper()[0]
scrape_archive()



if __name__ == "__main__":
    #iterate_dump()


    '''
    #set a start and end date
    start_date = {'year': 2014, 'month':2, 'day':21}
    end_date = {'year': 2014, 'month':2, 'day': 21}
    #iterate_dump()
    one_drudge_page_dump()
    '''
         
